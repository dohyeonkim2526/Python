import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LogisticRegression



trip_df=pd.read_csv('C:/Users/user/Desktop/　　　/2020-1학기/금_데이터마이닝/SF_bike_data/trip.csv')
station_df=pd.read_csv('C:/Users/user/Desktop/　　　/2020-1학기/금_데이터마이닝/SF_bike_data/station.csv')
weather_df=pd.read_csv('C:/Users/user/Desktop/　　　/2020-1학기/금_데이터마이닝/SF_bike_data/weather.csv')

trip_df.head()
trip_df.dtypes

trip_df.duration.describe()

#Change duration from sec to min
trip_df.duration /= 60
trip_df.duration.describe()

trip_df.duration.head()

#370분(평균) 이하의 값만 trip time으로 사용한다.
trip_df2=trip_df[trip_df.duration <= 370]

#convert start_date, end_date to datetime
#start_date와 end_date가 같으므로 start_date에 대해서만 year,month,day 정보 추출
trip_df2.start_date=pd.to_datetime(trip_df2.start_date, format='%m/%d/%Y %H:%M')
trip_df2['date'] = trip_df2.start_date.dt.date

#해당 날짜에 얼마나 많은 trip이 일어났는지 알아보기
dates={}
for i in trip_df2.date:
    if i not in dates:
        dates[i] = 1
    else:
        dates[i] += 1

#dates안에 있는 정보들을 가지고 training을 진행할 것이다.
data = pd.DataFrame.from_dict(dates,'index')
data['date'] = data.index #데이터프레임 data의 해당 인덱스가 날자를 의미하므로, 인덱스를 통해 'date'에 관한 열 생성
data.columns=['trips', 'date']

#train set생성
train=data[:]
train.reset_index(drop=True, inplace=True)


#_______________________________________________________________________________________________________________________#

weather_df.date=pd.to_datetime(weather_df.date, format='%m/%d/%Y')
weather_df['date'] = weather_df.date.dt.date

train.shape
weather_df.shape

#weather 안에 있는 zipcode확인
weather_df.zip_code.unique()

for zip in weather_df.zip_code.unique():
    print(weather_df[weather_df.zip_code == zip].isnull().sum())
    print()

#zip_code에 대해서 확인해봤을 때, zip_code='94107'에서 null 값의 수가 적다는 것을 알 수 있었고,
#zip_code = 94107에 대해서 train을 진행할 것임
weather=weather_df[weather_df.zip_code==94107]

weather.events.unique()

#'rain'과 'Rain'에 대해서 다른 경우로 나눠져 있다는 것을 알 수 있는데
#비가 오는 동일한 날씨이므로 'rain'을 'Rain으로 바꿔주고
#null값에 대해서는 보통인 날씨 'Normal'로 값을 바꿔준다.
weather.loc[weather.events=='rain','events']="Rain"
weather.loc[weather.events.isnull(), 'events']="Normal"

weather.events

events=pd.get_dummies(weather.events) #해당날씨이면 1, 아니면 0

#기존의 weather과 events(pd.get_dummis) 데이터프레임 합침
weather=weather.merge(events, left_index=True, right_index=True)

#사용하지 않는 events, zip_code에 대한 열 제거
weather=weather.drop(['events','zip_code'],1)


from scipy.stats.stats import pearsonr
print (pearsonr(weather.max_wind_Speed_mph[weather.max_gust_speed_mph >= 0], 
               weather.max_gust_speed_mph[weather.max_gust_speed_mph >= 0]))

#weather 데이터 프레임을 보면 max_gust에 대해서 null값이 있다는 것을 알 수 있다.
#상관관계 분석(pearsonr)을 통해 wind(바람)과 gust(세찬 바람)은 서로 관련이 있다는 것을 알았으므로, max_gust의 null값에 대해서
#max_wind값을 이용하여 채워줄 것이다.

weather.loc[weather.max_gust_speed_mph.isnull(), 'max_gust_speed_mph'] = weather.groupby('max_wind_Speed_mph').max_gust_speed_mph.apply(lambda x: x.fillna(x.median()))

weather.isnull().sum() #null값이 모두 채워진것을 확인
weather.precipitation_inches = pd.to_numeric(weather.precipitation_inches, errors = 'coerce') #강수량(precipitation_inch) 데이터 타입을 numeric으로 수정함

#강수량이 True인 것은 비가 왔음을 의미하므로, 강수량 null에 대해서 강수량의 평균값으로 채워준다.
weather.loc[weather.precipitation_inches.isnull(), 
            'precipitation_inches'] = weather[weather.precipitation_inches.notnull()].precipitation_inches.median()

train=pd.merge(weather, train, on='date')
train.head()

#_______________________________________________________________________________________________________________________

len(station_df.name.unique()) #station_df에 있는 station의 종류가 총70개이고, 중복된 것이 없다는 것을 확인

station_df.installation_date=pd.to_datetime(station_df.installation_date, format='%m/%d/%Y')
station_df['installation_date'] = station_df.installation_date.dt.date

#train DataFrame의 date를 통해, 해당 날짜 이전에 설치가 되어있던 docks의 수를 파악함 
total_docks = []
for day in train.date:
    total_docks.append(sum(station_df[station_df.installation_date <= day].dock_count))

train['total_docks'] = total_docks


#_______________________________________________________________________________________________________________________

#가지고 있는 train 데이터 셋의 date 기간동안 휴일을 계산함
from pandas.tseries.holiday import USFederalHolidayCalendar

calendar = USFederalHolidayCalendar()
holidays = calendar.holidays(start=train.date.min(), end=train.date.max())

#holidays를 제외한 business day찾기
from pandas.tseries.offsets import CustomBusinessDay 
business_days = CustomBusinessDay(calendar=USFederalHolidayCalendar())
business_days = pd.date_range(start=train.date.min(), end=train.date.max(), freq=business_days)

business_days = pd.to_datetime(business_days, format='%Y/%m/%d').date
holidays = pd.to_datetime(holidays, format='%Y/%m/%d').date

train['business_day'] = train.date.isin(business_days)
train['holiday'] = train.date.isin(holidays)

#business_day와 holiday의 (True,False)를 (1,0)으로 바꿔준다.
train.business_day = train.business_day.map(lambda x: 1 if x == True else 0)
train.holiday = train.holiday.map(lambda x: 1 if x == True else 0)

#date의 형식 (year,month,day)를 나눠서 열로 저장한다.
train['year'] = pd.to_datetime(train['date']).dt.year
train['month'] = pd.to_datetime(train['date']).dt.month
train['weekday'] = pd.to_datetime(train['date']).dt.weekday



#_______________________________________________________________________________________________________________________

#해당 날에 몇 개의 trip이 발생했는지를 알아내야 하므로 train['trip']을 정답 label로 저장한다.
labels = train.trips
train = train.drop(['trips', 'date'], 1)


#_______________________________________________________________________________________________________________________
#train, test data set 준비
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.2, random_state = 2)


#성능이 더 뛰어난 훈련을 진행하기 위해서 15-fold에 대해서 교차 검증을 시행할 계획이다.
from sklearn.model_selection import cross_val_score

#trip에 대한 예측이 얼마나 빗나갔는지 알기 위함

import numpy as np

def score(clf):
    scores = cross_val_score(clf, X_train, y_train, cv=15, scoring = 'neg_median_absolute_error')
    print (np.median(scores) * -1)

model1=RandomForestRegressor(n_estimators = 55,
                            min_samples_leaf = 3,
                            random_state = 2)

score(model1)

model2=GradientBoostingRegressor(learning_rate = 0.12,
                                 n_estimators = 150,
                                 max_depth = 8,
                                 min_samples_leaf = 1,
                                 random_state = 2)
score(model2)

model3=DecisionTreeRegressor(min_samples_leaf=3,
                             max_depth=8,
                             random_state=2)
score(model3)

model4=AdaBoostRegressor(n_estimators=100,
                         learning_rate = 0.1,
                         loss = 'linear',
                         random_state=2)

score(model4)



# import warnings
# warnings.filterwarnings("ignore")

# random_state = 2
# params = {
#         'eta': 0.15,
#         'max_depth': 6,
#         'min_child_weight': 2,
#         'subsample': 1,
#         'colsample_bytree': 1,
#         'verbose_eval': True,
#         'seed': random_state,
#     }


#_______________________________________________________________________________________________________________________

# from sklearn.model_selection import KFold
from sklearn.metrics import median_absolute_error
# n = 15
# scores = [] #각 폴더마다 mean_absolute_Error의 합계
# model_score = []

# k_fold=KFold(n_splits=15,shuffle=True, random_state=10)


# for i, (train_indices, test_indices) in enumerate(k_fold.split(X_train)):
#     clf=model1
#     Xtr, Xval = X_train.iloc[train_indices], X_train.iloc[test_indices]
#     Ytr, Yval = y_train.iloc[train_indices], y_train.iloc[test_indices]
#     clf.fit(Xtr, Ytr)
#     y_pred=clf.predict(Xval)
#     cv_score=median_absolute_error(Yval, y_pred)
#     scores.append(cv_score)
    
# score=np.median(scores)
# model_scores.append(score)


#k-fold 없이
clf1=model1.fit(X_train, y_train)
clf2=model2.fit(X_train,y_train)

clf1_pred=clf1.predict(X_test)
clf2_pred=clf2.predict(X_test)

preds=clf1_pred*0.52 + clf2_pred*0.48

print("Daily error of trip count:", median_absolute_error(y_test, preds))

labels.describe()

y_test.reset_index(drop=True, inplace=True)

import matplotlib.pyplot as plt
plt.figure(figsize=(8,5))
plt.plot(preds)
plt.plot(y_test)
plt.legend(['Pred','Actual'])
plt.xlabel("Predicted Date", fontsize=16)
plt.ylabel("Number of Trips", fontsize=16)
plt.title("Comparison of Predicted and Actual Date", fontsize=16)
plt.show()


#_______________________________________________________________________________________________________________________

#model1,2의 feature들 간의 중요도를 비교해봄

# def f_imp(model, model_name):
#     imp=model.feature_importances_
#     std=np.std([model.feature_importances_ for feature in model.estimators_], axis=0)
#     ind=np.argsort((imp)[::-1]*1000)
    
#     plt.figure(figsize=(8,5))
#     plt.title("Feature importances of " + model_name)
#     plt.bar(range(X_train.shape[1]), imp[ind], color='r', align='center')
#     plt.xlim([-1, X_train.shape[1]])
#     plt.xticks(range(X_train.shape[1]),ind)
#     plt.show()

print("Feature rank:")

count=0
for feature in X_train:
    print(count, feature)
    count += 1

# f=f_imp(model1, "Random Forest Regressor")
# f2=f_imp(model2, "Gradient Boosting Regressor")


import numpy as np

m1_feature=[]
m2_feature=[]
feature=np.array(model1.feature_importances_)
feature=np.argsort(-feature)[:10]

print("Feature Importance of model1")

for i in range(0,10):
    ind=feature[i]
    print("%d번째로 중요한 feature: " %(i+1))
    print(X_train.columns.values[ind])
    m1_feature.append(X_train.columns.values[ind])
    
feature2=np.array(model2.feature_importances_)
feature2=np.argsort(-feature2)[:10]

print("Feature Importance of model2")

for i in range(0,10):
    ind=feature2[i]
    print("%d번째로 중요한 feature: " %(i+1))
    print(X_train.columns.values[ind])
    m2_feature.append(X_train.columns.values[ind])

m1_feature    
m2_feature

두 모델에서의 상위 feature importance가 유사하다는 것을 알 수 있었다.





















data.dtypes


#trip_df2에서 data[date]와 같은 내용 count해서 data[trip]에 저장하기
count={}
for i in trip_df2.date:
    
            
        
    if trip_df2.date == data.date:
        dataes[i] += 1
        if i not in count:
            count[i] = 1
        else:
            count[i] += 1

data['trips'] = data.ix[:,0]












